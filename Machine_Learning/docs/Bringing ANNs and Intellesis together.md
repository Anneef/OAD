## Bringing ANNs and Intellesis together

Training artificial neural networks (ANN) to tackle semantic segmentation problems has become a very popular task and an increasing number of solutions has become available that require little technical understanding to train ANNs. Of course, this statement does not hold for models highly optimized for specific use cases where solving the problem at hand requires a high amount of experience, creativity and some “magic”. However, in many cases it is possible to achieve decent results with widely used ANN model architectures, a sufficient amount of data and enough compute power.

While about a decade ago solving computer vision tasks required an expert to design algorithms and understand the math behind them, ANNs and simple-to-use frameworks like [TensorFlow](https://www.tensorflow.org/) and [PyTorch](https://pytorch.org/) allow learning those algorithms from data and therefore can be treated as a black-box even achieving super-human performance in some cases. This simplicity and the high expressiveness of neural network models are presumably the reason that ANNs have become extremely popular and there is literally no way around this technology in research as well as industry. It should, however, be mentioned here that it is still extremely helpful to understand what happens behind the scenes.

### The challenges of deployment

While in research the focus mostly lies on training and evaluating models, deployment is a different issue that poses different challenges. Especially in microscopy there usually are not only captured small images but stacks of images, time series of images or tiled scenes that are stitched to one large image of multiple or even hundreds of gigabytes. The training process, in contrast, is typically performed on small image crops (e.g. 256 x 256) such that the data for the calculations fits into the GPU or main memory of the employed machine. Applying a trained model on considerably larger images as mentioned above brings about several pitfalls that need to be handled.

The [ZEN Intellesis](https://www.zeiss.com/microscopy/int/products/microscope-software/zen-intellesis-image-segmentation-by-deep-learning.html) infrastructure provides a solution for that problem. It applies a sophisticated tiling strategy that transforms multi-dimensional image captures – as they are generated by ZEISS microscopes – to image tiles of shape (height x width x color channels) where height and width are defined by the applied ANN model. Finally, it performs inference with the model and reassembles the resulting segmentation masks to fit the initially captured image. It therefore offers a solution to apply a trained ANN on large images without worrying too much about the required memory and the size of the image.

### czmodel – From TensorFlow to Intellesis

Starting with ZEN Blue 3.2 it will be possible to run prediction with custom segmentation models implemented in TensorFlow 2. To be compatible with the Intellesis infrastructure, a TensorFlow/Keras model has to comply with our specification and some meta data must be provided that is described in detail in the official ANN model specification in the project description of our [czmodel package](https://pypi.org/project/czmodel/). This blog post introduces a python library called czmodel that offers a set of tools to easily convert a trained TensorFlow/Keras model to a CZMODEL file that can be directly loaded into ZEN Intellesis. Generating a CZMODEL file this way is just a matter of a few lines of code and can therefore easily be integrated into any training pipeline as well as custom Apeer modules.

In this blog post we assume that there is some kind of training pipeline in place that generates trained TensorFlow/Keras models as instances of `tensorflow.keras.Model`. For a simple example of such a pipeline see the attached IPython notebook in our [demo project](https://notebooks.azure.com/sebastian-soyer/projects/czmodel). Note that the notebook is not to be understood as a best practice guide for training ANN models but rather illustrates the use of the [czmodel](https://pypi.org/project/czmodel/) library with a very simple model generation process that will usually be more sophisticated.

The entire ´czmodel´ package operates on so called ´ModelSpec´ objects that contain all information needed to convert a TensorFlow/Keras model to CZMODEL. Specifically, it contains the trained model itself and a metadata object called ´ModelMetadata´ that reflects the JSON metadata described in the [ANN model specification](https://pypi.org/project/czmodel/) as a python class. Thus, the typical workflow of converting a TensorFlow/Keras model to CZMODEL is to create a `ModelMetadata` object, wrap it into a `ModelSpec` object together with the corresponding TensorFlow/Keras model and, finally, provide the ModelSpec object to the conversion function that generates the CZMODEL file.

The `czmodel` package supports three different ways to generate a CZMODEL.

#### Convert a loaded TensorFlow/Keras model to CZMODEL using a ModelSpec

This scenario reflects exactly the workflow described above. The ModelMetadata and ModelSpec objects for a loaded TensorFlow/Keras model named “model” are created as follows:

```python
from czmodel.model_metadata import ModelMetadata, ModelSpec
from czmodel.convert import convert_from_model_spec

# Create the model metadata
model_metadata = ModelMetadata.from_params(name='DNNModelFromKeras',
                         color_handling='ConvertToMonochrome',
                         pixel_type='Gray16',
                         classes=["Background", "Interesting Object", "Foreground"],
                         border_size=90,
                         license_file="C:\\some\\path\\to\\a\\LICENSE.txt")

# Create the model spec
model_spec = ModelSpec(model=model, model_metadata=model_metadata)

# Convert the model
convert_from_model_spec(model_spec, 'Output path', 'Model Name')
```

#### Convert a SavedModel on disk to CZMODEL using a ModelSpec

In this case we assume the TensorFlow/Keras model not to reside in the main memory but on disk in the SavedModel format. Let “path” be the path to the SavedModel folder. We can then apply the same procedure as shown above with loaded models but instantiate the ModelSpec object with the path of the model instead of the loaded model:

```python
model_spec = ModelSpec(model=path, model_metadata=model_metadata)
```

The `czmodel` library will automatically load the model from the given path before wrapping it into a CZMODEL.

Convert a SavedModel on disk to CZMODEL by providing a JSON metadata file
This functionality replicates the import functionality of ZEN Intellesis to load models via a [JSON specification](https://pypi.org/project/czmodel/). If models should e.g. be exchanged with third parties, it is usually easier to deliver CZMODEL files that contain everything needed to run the model within Intellesis than exchanging JSON files with the corresponding SavedModel folders that would, nonetheless, need to be bundled another way.
The `czmodel` library provides a simple conversion routine that does all this in one line:

```python
from czmodel.convert import convert_from_json_spec

convert_from_json_spec('Path to JSON file', 'Output path', 'Model Name')
```

Note that the path to the model is implicitly defined by the provided JSON file.

#### Adding pre-processing and setting the tile size

Most trained models require some kind of pre-processing (e.g. normalization/standardization) to transform the input data to the distribution the model was actually trained on. The Intellesis inference engine requires this to be part of the model while – depending on the training infrastructure - this is not necessarily the case during training.

All conversion functions in the `czmodel` library wrap the provided model into a deployment model if pre-processing is applied or the spatial dimensions should be reset. They additionally accept a list of TensorFlow/Keras layers as a `preprocessing` argument that will be prepended to the provided model. Additionally, setting the parameter `spatial_dims` allows to redefine the spatial dimensions of the expected model inputs that implicitly defines the size of the tiles provided to the inference engine. Note that this is only possible for models that are invariant w.r.t to the spatial dimensions.

While the `czmodel.util.preprocessing` module provides a set of useful pre-processing layers, any TensorFlow/Keras layer can in theory be used for pre-processing. The following call generates a CZMODEL from the above defined ModelSpec object, appends a RgbToBgr pre-processing layer to transform the input images to a different color space and sets the expected size of the input images to 512 x 512:

```python
from czmodel.util.preprocessing import RgbToBgr
convert_from_model_spec(model_spec,
        'Output path',
        'Model Name',
        Preprocessing= RgbToBgr(),
        spatial_dims=(512, 512))
```

#### Remarks

The current implementation of `czmodel` heavily relies on the SavedModel import/export functionality in TensorFlow 2. We therefore recommend to always use the latest version of TensorFlow 2 compatible to `czmodel` as there are continuously released improvements and bugfixes in the TensorFlow code base.
